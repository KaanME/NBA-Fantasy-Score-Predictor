{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_n_dict = pickle.load(open(\"last_n_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>player_name</th>\n",
       "      <th>game_date</th>\n",
       "      <th>team</th>\n",
       "      <th>opp</th>\n",
       "      <th>home</th>\n",
       "      <th>wl</th>\n",
       "      <th>mp</th>\n",
       "      <th>fgm</th>\n",
       "      <th>fga</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>fg3m</th>\n",
       "      <th>fg3a</th>\n",
       "      <th>fg3_pct</th>\n",
       "      <th>ftm</th>\n",
       "      <th>fta</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>oreb</th>\n",
       "      <th>dreb</th>\n",
       "      <th>reb</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>tov</th>\n",
       "      <th>pf</th>\n",
       "      <th>pts</th>\n",
       "      <th>plus_minus</th>\n",
       "      <th>fscore</th>\n",
       "      <th>ast_pct</th>\n",
       "      <th>blk_pct</th>\n",
       "      <th>dreb_pct</th>\n",
       "      <th>oreb_pct</th>\n",
       "      <th>reb_pct</th>\n",
       "      <th>stl_pct</th>\n",
       "      <th>tov_pct</th>\n",
       "      <th>efg_pct</th>\n",
       "      <th>ts_pct</th>\n",
       "      <th>usg_pct</th>\n",
       "      <th>days_off</th>\n",
       "      <th>game_count</th>\n",
       "      <th>fscore_exp</th>\n",
       "      <th>above_avg_fscore</th>\n",
       "      <th>wl_l5</th>\n",
       "      <th>mp_l5</th>\n",
       "      <th>fgm_l5</th>\n",
       "      <th>fga_l5</th>\n",
       "      <th>fg_pct_l5</th>\n",
       "      <th>fg3m_l5</th>\n",
       "      <th>fg3a_l5</th>\n",
       "      <th>fg3_pct_l5</th>\n",
       "      <th>ftm_l5</th>\n",
       "      <th>fta_l5</th>\n",
       "      <th>ft_pct_l5</th>\n",
       "      <th>oreb_l5</th>\n",
       "      <th>dreb_l5</th>\n",
       "      <th>reb_l5</th>\n",
       "      <th>ast_l5</th>\n",
       "      <th>stl_l5</th>\n",
       "      <th>blk_l5</th>\n",
       "      <th>tov_l5</th>\n",
       "      <th>pf_l5</th>\n",
       "      <th>pts_l5</th>\n",
       "      <th>plus_minus_l5</th>\n",
       "      <th>fscore_l5</th>\n",
       "      <th>ast_pct_l5</th>\n",
       "      <th>blk_pct_l5</th>\n",
       "      <th>dreb_pct_l5</th>\n",
       "      <th>oreb_pct_l5</th>\n",
       "      <th>reb_pct_l5</th>\n",
       "      <th>stl_pct_l5</th>\n",
       "      <th>tov_pct_l5</th>\n",
       "      <th>efg_pct_l5</th>\n",
       "      <th>ts_pct_l5</th>\n",
       "      <th>usg_pct_l5</th>\n",
       "      <th>days_off_l5</th>\n",
       "      <th>tm_pace_l5</th>\n",
       "      <th>tm_off_eff_l5</th>\n",
       "      <th>opp_pace_l5</th>\n",
       "      <th>opp_off_eff_l5</th>\n",
       "      <th>opp_def_eff_l5</th>\n",
       "      <th>opp_days_off_l5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>A.J. Guyton</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>CHI</td>\n",
       "      <td>SAC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.150668</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.667683</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.150668</td>\n",
       "      <td>4.0</td>\n",
       "      <td>95.770863</td>\n",
       "      <td>101.249631</td>\n",
       "      <td>95.643037</td>\n",
       "      <td>109.291045</td>\n",
       "      <td>96.012694</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>Greg Foster</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>LAL</td>\n",
       "      <td>POR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.75</td>\n",
       "      <td>49.688150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.641026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.320513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.422037</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.412162</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>49.688150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.641026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.320513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.422037</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.673868</td>\n",
       "      <td>84.260078</td>\n",
       "      <td>91.393504</td>\n",
       "      <td>89.279910</td>\n",
       "      <td>110.222112</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>Devean George</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>LAL</td>\n",
       "      <td>POR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-9</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>66.250866</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.576923</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>66.250866</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.673868</td>\n",
       "      <td>84.260078</td>\n",
       "      <td>91.393504</td>\n",
       "      <td>89.279910</td>\n",
       "      <td>110.222112</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>Brian Shaw</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>LAL</td>\n",
       "      <td>POR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>11.75</td>\n",
       "      <td>32.268227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.170940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.085470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.604651</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.902062</td>\n",
       "      <td>22.790298</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.762195</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.75</td>\n",
       "      <td>32.268227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.170940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.085470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.604651</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.902062</td>\n",
       "      <td>22.790298</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.673868</td>\n",
       "      <td>84.260078</td>\n",
       "      <td>91.393504</td>\n",
       "      <td>89.279910</td>\n",
       "      <td>110.222112</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>Gerard King</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>WAS</td>\n",
       "      <td>ORL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11.25</td>\n",
       "      <td>16.947961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.279720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.639860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.246018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>16.947961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.279720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.639860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.246018</td>\n",
       "      <td>4.0</td>\n",
       "      <td>85.095821</td>\n",
       "      <td>83.795707</td>\n",
       "      <td>95.770863</td>\n",
       "      <td>111.871875</td>\n",
       "      <td>103.428715</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season    player_name   game_date team  opp  home  wl  mp  fgm  fga  \\\n",
       "0    2000    A.J. Guyton  2000-10-31  CHI  SAC     1   0   4    0    1   \n",
       "1    2000    Greg Foster  2000-10-31  LAL  POR     0   1   4    1    1   \n",
       "2    2000  Devean George  2000-10-31  LAL  POR     0   1   3    1    2   \n",
       "3    2000     Brian Shaw  2000-10-31  LAL  POR     0   1  15    2    3   \n",
       "4    2000    Gerard King  2000-10-31  WAS  ORL     0   0  13    3    3   \n",
       "\n",
       "   fg_pct  fg3m  fg3a  fg3_pct  ftm  fta  ft_pct  oreb  dreb  reb  ast  stl  \\\n",
       "0   0.000     0     1      0.0    0    0     0.0     0     0  0.0    0    0   \n",
       "1   1.000     0     0      0.0    0    0     0.0     0     1  1.0    1    0   \n",
       "2   0.500     1     2      0.5    0    0     0.0     0     0  0.0    0    0   \n",
       "3   0.667     1     1      1.0    2    2     1.0     0     1  1.0    3    0   \n",
       "4   1.000     0     0      0.0    0    0     0.0     0     3  3.0    1    0   \n",
       "\n",
       "   blk  tov  pf  pts  plus_minus  fscore    ast_pct  blk_pct   dreb_pct  \\\n",
       "0    0    0   0    0          -1    0.00   0.000000      0.0   0.000000   \n",
       "1    0    0   0    2           3    4.75  49.688150      0.0  30.641026   \n",
       "2    0    2   1    3          -9    2.50   0.000000      0.0   0.000000   \n",
       "3    0    3   1    7           3   11.75  32.268227      0.0   8.170940   \n",
       "4    0    0   2    6           5   11.25  16.947961      0.0  25.279720   \n",
       "\n",
       "   oreb_pct    reb_pct  stl_pct    tov_pct   efg_pct    ts_pct    usg_pct  \\\n",
       "0       0.0   0.000000      0.0   0.000000  0.000000  0.000000  12.150668   \n",
       "1       0.0  15.320513      0.0   0.000000  1.000000  1.000000  12.422037   \n",
       "2       0.0   0.000000      0.0  50.000000  0.750000  0.750000  66.250866   \n",
       "3       0.0   4.085470      0.0  43.604651  0.833333  0.902062  22.790298   \n",
       "4       0.0  12.639860      0.0   0.000000  1.000000  1.000000  10.246018   \n",
       "\n",
       "   days_off  game_count  fscore_exp  above_avg_fscore  wl_l5  mp_l5  fgm_l5  \\\n",
       "0       4.0           1   10.667683                 0    0.0    4.0     0.0   \n",
       "1       4.0           1   15.412162                 0    1.0    4.0     1.0   \n",
       "2       4.0           1   10.576923                 0    1.0    3.0     1.0   \n",
       "3       4.0           1   30.762195                 0    1.0   15.0     2.0   \n",
       "4       4.0           1   20.625000                 0    0.0   13.0     3.0   \n",
       "\n",
       "   fga_l5  fg_pct_l5  fg3m_l5  fg3a_l5  fg3_pct_l5  ftm_l5  fta_l5  ft_pct_l5  \\\n",
       "0     1.0      0.000      0.0      1.0         0.0     0.0     0.0        0.0   \n",
       "1     1.0      1.000      0.0      0.0         0.0     0.0     0.0        0.0   \n",
       "2     2.0      0.500      1.0      2.0         0.5     0.0     0.0        0.0   \n",
       "3     3.0      0.667      1.0      1.0         1.0     2.0     2.0        1.0   \n",
       "4     3.0      1.000      0.0      0.0         0.0     0.0     0.0        0.0   \n",
       "\n",
       "   oreb_l5  dreb_l5  reb_l5  ast_l5  stl_l5  blk_l5  tov_l5  pf_l5  pts_l5  \\\n",
       "0      0.0      0.0     0.0     0.0     0.0     0.0     0.0    0.0     0.0   \n",
       "1      0.0      1.0     1.0     1.0     0.0     0.0     0.0    0.0     2.0   \n",
       "2      0.0      0.0     0.0     0.0     0.0     0.0     2.0    1.0     3.0   \n",
       "3      0.0      1.0     1.0     3.0     0.0     0.0     3.0    1.0     7.0   \n",
       "4      0.0      3.0     3.0     1.0     0.0     0.0     0.0    2.0     6.0   \n",
       "\n",
       "   plus_minus_l5  fscore_l5  ast_pct_l5  blk_pct_l5  dreb_pct_l5  oreb_pct_l5  \\\n",
       "0           -1.0       0.00    0.000000         0.0     0.000000          0.0   \n",
       "1            3.0       4.75   49.688150         0.0    30.641026          0.0   \n",
       "2           -9.0       2.50    0.000000         0.0     0.000000          0.0   \n",
       "3            3.0      11.75   32.268227         0.0     8.170940          0.0   \n",
       "4            5.0      11.25   16.947961         0.0    25.279720          0.0   \n",
       "\n",
       "   reb_pct_l5  stl_pct_l5  tov_pct_l5  efg_pct_l5  ts_pct_l5  usg_pct_l5  \\\n",
       "0    0.000000         0.0    0.000000    0.000000   0.000000   12.150668   \n",
       "1   15.320513         0.0    0.000000    1.000000   1.000000   12.422037   \n",
       "2    0.000000         0.0   50.000000    0.750000   0.750000   66.250866   \n",
       "3    4.085470         0.0   43.604651    0.833333   0.902062   22.790298   \n",
       "4   12.639860         0.0    0.000000    1.000000   1.000000   10.246018   \n",
       "\n",
       "   days_off_l5  tm_pace_l5  tm_off_eff_l5  opp_pace_l5  opp_off_eff_l5  \\\n",
       "0          4.0   95.770863     101.249631    95.643037      109.291045   \n",
       "1          4.0   97.673868      84.260078    91.393504       89.279910   \n",
       "2          4.0   97.673868      84.260078    91.393504       89.279910   \n",
       "3          4.0   97.673868      84.260078    91.393504       89.279910   \n",
       "4          4.0   85.095821      83.795707    95.770863      111.871875   \n",
       "\n",
       "   opp_def_eff_l5  opp_days_off_l5  \n",
       "0       96.012694              4.0  \n",
       "1      110.222112              4.0  \n",
       "2      110.222112              4.0  \n",
       "3      110.222112              4.0  \n",
       "4      103.428715              4.0  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last5 = last_n_dict[5].loc[~last5.fscore_exp.isnull()]\n",
    "last5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Xy_linreg(data_dict, n=5):\n",
    "    data = data_dict[n]\n",
    "    x = data.loc[:,'days_off':].loc[~data.fscore_exp.isnull()].drop('above_avg_fscore',1)\n",
    "    x['home'] = data.home\n",
    "    y = data.fscore.loc[~data.fscore_exp.isnull()]\n",
    "    return x,y\n",
    "        \n",
    "def get_Xy_logreg(data_dict, n=5):\n",
    "    data = data_dict[n]\n",
    "    x = data.loc[:,('wl_l'+str(n)):]\n",
    "    x['home'] = data.home\n",
    "    y = data.above_avg_fscore\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def year_train_test(data_dict, n=5, year = 2017):\n",
    "    data = data_dict[n]\n",
    "    x_train = data.loc[:,'days_off':].loc[data.season != year]\n",
    "    x_train['home'] = data.home.loc[data.season != year]\n",
    "    y_train = data.loc[data.season != year].fscore\n",
    "\n",
    "    x_test = data.loc[:,'days_off':].loc[data.season == year]\n",
    "    x_test['home'] = data.home.loc[data.season == year]\n",
    "    y_test = data.loc[data.season == year].fscore\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def szn_train_test(data_dict, n = 5, year = 2017):\n",
    "    data = data_dict[n]\n",
    "    X = data.loc[:,'days_off':].loc[data.season == year]\n",
    "    X['home'] = data.home.loc[data.season == year]\n",
    "    y = data.loc[data.season == year].fscore\n",
    "    return train_test_split(X,y, test_size = .2, random_state=1)\n",
    "\n",
    "def plyr_train_test(data_dict, player, n = 5):\n",
    "    data = data_dict[n]\n",
    "    player_df = data.loc[data.player_name == player]\n",
    "    x_train = player_df.loc[:,'days_off':][:round((.9*len(player_df)))]\n",
    "    x_train['home'] = data.home.loc[data.player_name == player]\n",
    "    y_train = player_df.fscore[:round((.9*len(player_df)))]\n",
    "\n",
    "    x_test = player_df.loc[:,'days_off':][round((.9*len(player_df))):]\n",
    "    x_test['home'] = data.home.loc[data.player_name == player]\n",
    "    y_test = player_df.fscore[round((.9*len(player_df))):]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = szn_train_test_log(last_n_dict)\n",
    "X,y = get_Xy_linreg(last_n_dict,n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore_exp\n",
      "mp_l5\n",
      "fgm_l5\n",
      "fga_l5\n",
      "ftm_l5\n",
      "fta_l5\n",
      "dreb_l5\n",
      "tov_l5\n",
      "pts_l5\n",
      "fscore_l5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression,chi2,f_classif\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "kbest = SelectKBest(f_regression, k=10)\n",
    "\n",
    "kbest.fit(X,y)\n",
    "for label in X.columns[kbest.get_support()]:\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_picked = ['mp_l5', 'pts_l5','ast_l5','reb_l5','stl_l5','blk_l5','tov_l5','fg3m_l5','opp_def_eff_l5','tm_pace_l5', 'home']\n",
    "\n",
    "fts_kbest = X.columns[kbest.get_support()]\n",
    "Xp = X[fts_picked]\n",
    "Xk_ = X[fts_kbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [618, 19946]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-201-dfcacbd45c0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXk_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlin_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlin_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 482\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [618, 19946]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(Xk_train)\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_scaled, y_train)\n",
    "y_pred = lin_reg.predict(X_scaled)\n",
    "lin_mse = mean_squared_error(y_train, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.4992489  0.49936694 0.49840361 0.49837309 0.49764901 0.49795058\n",
      " 0.50031652 0.49996457 0.49895442 0.49914278]\n",
      "Mean: 0.4989370417389479\n",
      "Standard deviation: 0.0008073519495835101\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(scaled_ols(), X_train, y_train,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_rmse(X,y, model, cv=5):\n",
    "    scores = cross_val_score(model, X, y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=cv)\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "def avg_score(X,y, model,score=\"neg_mean_squared_error\", cv=5):\n",
    "    scores = cross_val_score(model, X, y,\n",
    "                             scoring=score, cv=cv)\n",
    "    if score == \"neg_mean_squared_error\":\n",
    "        scores = np.sqrt(-scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def scaled_ridge(alpha = 1):\n",
    "    sc_ridge = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Ridge(alpha = alpha)),\n",
    "        ]) \n",
    "    return sc_ridge\n",
    "\n",
    "def scaled_lasso(alpha = 1):\n",
    "    sc_ridge = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Lasso(alpha = alpha)),\n",
    "        ]) \n",
    "    return sc_ridge\n",
    "\n",
    "def scaled_ols(alpha = 1):\n",
    "    sc_ols = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", LinearRegression()),\n",
    "        ]) \n",
    "    return sc_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004818129114744307"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score(X_train, y_train, scaled_ols(), score='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0004418207302934185"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score(X_train, y_train, scaled_lasso(),score='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004804162802337442"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score(X_train, y_train, scaled_ridge(), score='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 1\n",
      "Mean: 6.134085009452781 \n",
      "\n",
      "last 3\n",
      "Mean: 6.092729016140991 \n",
      "\n",
      "last 5\n",
      "Mean: 6.081595070954097 \n",
      "\n",
      "last 7\n",
      "Mean: 6.075042739299618 \n",
      "\n",
      "last 10\n",
      "Mean: 6.065080003786276 \n",
      "\n",
      "last 15\n",
      "Mean: 6.065762320901785 \n",
      "\n",
      "last 20\n",
      "Mean: 6.072116379481036 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(last_n_dict.keys()):\n",
    "    X_train,X_test,y_train,y_test = szn_train_test(last_n_dict, year = 2017, n=key)\n",
    "    rmse = avg_score(X_train,y_train,scaled_ridge(),cv=10)\n",
    "    print('last',key)\n",
    "    print(\"Mean: {}\".format(rmse),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 1\n",
      "Mean: 6.32294228829534 \n",
      "\n",
      "last 3\n",
      "Mean: 6.2836796882750825 \n",
      "\n",
      "last 5\n",
      "Mean: 6.270739035669548 \n",
      "\n",
      "last 7\n",
      "Mean: 6.261674275853071 \n",
      "\n",
      "last 10\n",
      "Mean: 6.2459529991587965 \n",
      "\n",
      "last 15\n",
      "Mean: 6.246098182115103 \n",
      "\n",
      "last 20\n",
      "Mean: 6.250059683483644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(last_n_dict.keys()):\n",
    "    X_train,X_test,y_train,y_test = szn_train_test(last_n_dict,  year = 2017, n=key)\n",
    "    rmse = avg_rmse(X_train,y_train,scaled_lasso(),10)\n",
    "    print('last',key)\n",
    "    print(\"Mean: {}\".format(rmse),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 1\n",
      "Mean: 6.138877705824542 \n",
      "\n",
      "last 3\n",
      "Mean: 6.094288730841071 \n",
      "\n",
      "last 5\n",
      "Mean: 6.099109629128705 \n",
      "\n",
      "last 7\n",
      "Mean: 6.081930873564024 \n",
      "\n",
      "last 10\n",
      "Mean: 6.065387586234768 \n",
      "\n",
      "last 15\n",
      "Mean: 6.067116361603868 \n",
      "\n",
      "last 20\n",
      "Mean: 6.076253316886084 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(last_n_dict.keys()):\n",
    "    X_train,X_test,y_train,y_test = szn_train_test(last_n_dict, n=key)\n",
    "    rmse = avg_rmse(X_train,y_train,scaled_ols())\n",
    "    print('last',key)\n",
    "    print(\"Mean: {}\".format(rmse),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels=19946 does not match number of samples=618",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-089ee43060b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtree_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtree_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtree_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[1;32m--> 236\u001b[1;33m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of labels=19946 does not match number of samples=618"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_scaled, y_train)\n",
    "y_pred = tree_reg.predict(X_scaled)\n",
    "tree_mse = mean_squared_error(y_train, y_pred)\n",
    "tree_mse = np.sqrt(tree_mse)\n",
    "\n",
    "scores = cross_val_score(tree_reg, X_scaled, y,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def scaled_tree():\n",
    "    sc_tree = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"tree\", DecisionTreeRegressor()),\n",
    "        ]) \n",
    "    return sc_tree\n",
    "\n",
    "def scaled_forest(max_depth = 2):\n",
    "    sc_forest = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"tree\", RandomForestRegressor(max_depth=max_depth)),\n",
    "        ]) \n",
    "    return sc_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 1\n",
      "Mean: 8.598612325534278 \n",
      "\n",
      "last 3\n",
      "Mean: 8.526124471615198 \n",
      "\n",
      "last 5\n",
      "Mean: 8.48284827512973 \n",
      "\n",
      "last 7\n",
      "Mean: 8.515429404781276 \n",
      "\n",
      "last 10\n",
      "Mean: 8.372662965264535 \n",
      "\n",
      "last 15\n",
      "Mean: 8.482357803716113 \n",
      "\n",
      "last 20\n",
      "Mean: 8.414108747127443 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(last_n_dict.keys()):\n",
    "    X_train,X_test,y_train,y_test = szn_train_test(last_n_dict, n=key, year = 2017)\n",
    "    rmse = avg_rmse(X_train,y_train,scaled_tree())\n",
    "    print('last',key)\n",
    "    print(\"Mean: {}\".format(rmse),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [12.55497949 12.03461567 12.70432672  8.3670684  15.77491986]\n",
      "Mean: 12.28718202768358\n",
      "Standard deviation: 2.359708499478915\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = plyr_train_test(last_n_dict, player='James Harden', n=key)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X_scaled = StandardScaler().fit_transform(X_train)\n",
    "forest_reg = RandomForestRegressor(max_depth=2)\n",
    "\n",
    "scores = cross_val_score(forest_reg, X_scaled, y_train,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=5)\n",
    "forest_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 1\n",
      "Mean: 8.231760707115157 \n",
      "\n",
      "last 3\n",
      "Mean: 8.180093036793783 \n",
      "\n",
      "last 5\n",
      "Mean: 8.304010656497164 \n",
      "\n",
      "last 7\n",
      "Mean: 8.645462883978642 \n",
      "\n",
      "last 10\n",
      "Mean: 8.478979996087538 \n",
      "\n",
      "last 15\n",
      "Mean: 8.380361200922133 \n",
      "\n",
      "last 20\n",
      "Mean: 8.362146107200923 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(last_n_dict.keys()):\n",
    "    X_train,X_test,y_train,y_test = szn_train_test(last_n_dict, n=key, year=2017)\n",
    "    rmse = avg_rmse(X_train,y_train,scaled_forest(2))\n",
    "    print('last',key)\n",
    "    print(\"Mean: {}\".format(rmse),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [618, 19946]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-be39467c0679>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                            scoring='neg_mean_squared_error')\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'score'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [618, 19946]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'max_features': [5,10,15,20], 'max_depth' : [2,3,4,5]}\n",
    "#    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_scaled, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
       "           max_features=10, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00319944, 0.        , 0.01522213, 0.00681516,\n",
       "       0.15995415, 0.        , 0.        , 0.09010569, 0.        ,\n",
       "       0.16832208, 0.07824394, 0.00273374, 0.        , 0.00273468,\n",
       "       0.00443496, 0.1909288 , 0.00227217, 0.        , 0.00078702,\n",
       "       0.00086054, 0.        , 0.00233895, 0.09765507, 0.01904201,\n",
       "       0.        , 0.00075184, 0.        , 0.        , 0.        ,\n",
       "       0.01249231, 0.        , 0.00054533, 0.10658274, 0.        ,\n",
       "       0.01266734, 0.00848346, 0.00195935, 0.        , 0.01021138,\n",
       "       0.0006557 , 0.        ])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.19092880350165445, 'ast_l20'),\n",
       " (0.1683220825565735, 'ftm_l20'),\n",
       " (0.15995414941371805, 'fga_l20'),\n",
       " (0.1065827364689308, 'usg_pct_l20'),\n",
       " (0.09765506598684684, 'fscore_l20'),\n",
       " (0.09010569366462207, 'fg3a_l20'),\n",
       " (0.0782439435466191, 'fta_l20'),\n",
       " (0.01904201158525199, 'ast_pct_l20'),\n",
       " (0.015222127469787202, 'mp_l20'),\n",
       " (0.012667341495342089, 'tm_pace_l20'),\n",
       " (0.012492314551856695, 'tov_pct_l20'),\n",
       " (0.010211380815692471, 'opp_def_eff_l20'),\n",
       " (0.008483455650691425, 'tm_off_eff_l20'),\n",
       " (0.006815159853795313, 'fgm_l20'),\n",
       " (0.0044349635017714, 'reb_l20'),\n",
       " (0.0031994446912351647, 'game_count'),\n",
       " (0.0027346788666891826, 'dreb_l20'),\n",
       " (0.0027337400706606936, 'ft_pct_l20'),\n",
       " (0.0023389519668537196, 'plus_minus_l20'),\n",
       " (0.002272165019277047, 'stl_l20'),\n",
       " (0.001959354210146042, 'opp_pace_l20'),\n",
       " (0.0008605430675756138, 'pf_l20'),\n",
       " (0.0007870197923208316, 'tov_l20'),\n",
       " (0.0007518440934789831, 'dreb_pct_l20'),\n",
       " (0.0006556961134159283, 'opp_days_off_l20'),\n",
       " (0.0005453320451934897, 'ts_pct_l20'),\n",
       " (0.0, 'wl_l20'),\n",
       " (0.0, 'stl_pct_l20'),\n",
       " (0.0, 'reb_pct_l20'),\n",
       " (0.0, 'pts_l20'),\n",
       " (0.0, 'oreb_pct_l20'),\n",
       " (0.0, 'oreb_l20'),\n",
       " (0.0, 'opp_off_eff_l20'),\n",
       " (0.0, 'home'),\n",
       " (0.0, 'fg_pct_l20'),\n",
       " (0.0, 'fg3m_l20'),\n",
       " (0.0, 'fg3_pct_l20'),\n",
       " (0.0, 'efg_pct_l20'),\n",
       " (0.0, 'days_off_l20'),\n",
       " (0.0, 'days_off'),\n",
       " (0.0, 'blk_pct_l20'),\n",
       " (0.0, 'blk_l20')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_importances, X_train.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.458615928751538"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test_prepared = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def szn_train_test_log(data_dict, n = 5, year = 2017):\n",
    "    data = data_dict[n]\n",
    "    X = data.loc[:,('wl_l'+str(n)):].loc[data.season == year]\n",
    "    X['home'] = data.home.loc[data.season == year]\n",
    "    y = data.loc[data.season == year].above_avg_fscore\n",
    "    return train_test_split(X,y, test_size = .2, random_state=1)\n",
    "\n",
    "def plyr_train_test_log(data_dict, player, n = 5):\n",
    "    data = data_dict[n]\n",
    "    player_df = data.loc[data.player_name == player]\n",
    "    X = player_df.loc[:,('wl_l'+str(n)):]\n",
    "    X['home'] = data.home.loc[data.player_name == player]\n",
    "    y = player_df.above_avg_fscore\n",
    "    \n",
    "    return train_test_split(X,y, test_size = .2, random_state=1)\n",
    "\n",
    "def year_train_test_log(data_dict, n=5, year = 2017):\n",
    "    data = data_dict[n]\n",
    "    x_train = data.loc[:,('wl_l'+str(n)):].loc[data.season != year]\n",
    "    x_train['home'] = data.home.loc[data.season != year]\n",
    "    y_train = data.loc[data.season != year].above_avg_fscore\n",
    "\n",
    "    x_test = data.loc[:,('wl_l'+str(n)):].loc[data.season == year]\n",
    "    x_test['home'] = data.home.loc[data.season == year]\n",
    "    y_test = data.loc[data.season == year].above_avg_fscore\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def scaled_log():\n",
    "    sc_log = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression()),\n",
    "        ]) \n",
    "    return sc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5789 6779]\n",
      " [4943 7422]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.46      0.50     12568\n",
      "          1       0.52      0.60      0.56     12365\n",
      "\n",
      "avg / total       0.53      0.53      0.53     24933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = year_train_test_log(last_n_dict, n = 10)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = scaled_log()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEpCAYAAABSuB9PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nt/8HkvoCAk0rB+sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28495ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5442601676720935\n",
      "AUC scores computed using 5-fold cross-validation: [0.5512855  0.55257079 0.56260058 0.56404457 0.55907784 0.55677902\n",
      " 0.54297581 0.55210313 0.54131217 0.55517626]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Compute cross-validated AUC scores: cv_auc\n",
    "cv_auc = cross_val_score(logreg,X_train,y_train,cv=10,scoring='roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
